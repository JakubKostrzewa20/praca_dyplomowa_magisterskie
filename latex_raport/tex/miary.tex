\newpage
\section{Metodyka badań}
Celem poniższego rozdziału jest przedstawienie specyfikacji sprzetowej, metod, narzędzi oraz przeprowadzonych badań porównawczych modeli głębokiego nauczania 
\subsection{Specyfikacja sprzętowa i środowiska programistycznego}
Wszystkie szkolenia oraz testowanie modeli zostały wykonane na sprzęcie o tych samych specyfikacjach, w celu zachowania obiektywności ekseprymentu. W badaniach wykorzystano komputer osobisty z konkretnymi specyfikacjami:
\begin{itemize}
	\item 
\end{itemize}
Do treningu oraz ewaluacji modeli wykorzystano narzędzia oraz biblioteki języka Python:
\begin{itemize}
	\item 
\end{itemize}
\subsection{Lista modeli wykorzystana w badaniu}
Poniżej przedstawiono listę modeli wybranych do badania architektur:
\begin{itemize}
	\item MobileNetV2
	\item MobileNetV3Small
	\item ViT-S/16
	\item ViT-B/16
	\item ResNet50
	\item ResNet30
	\item EfficientNetB0
	\item EfficientNetB3
\end{itemize}
\subsection{Strategia treningowa}
W ramach treningu wyszkolono osiem modeli, po dwie z przedstawionych 
\subsection{Augmentacja danych}
Z powodu niezbalansowanego zbioru danych zastosowano augmentacje danych, mającą na celu zapobieganie przeuczeniu. Zdjęcia dostarczane do modeli w ramach zbioru treningowego są poddawane przekształceniom, takim jak:
Poniżej przedstawiono kod warstwy odpowiadającej za augmentacje: 
\begin{lstlisting}[language=Python, caption={Kod warstwy odpowiadającej za augmentacje danych}, label={lst:paugmentacja}]
data_augmentation = tf.keras.Sequential(
	[
	layers.RandomFlip("horizontal"),
	layers.RandomRotation(0.2),
	layers.RandomContrast(0.2),
	layers.RandomZoom(0.2),
	]
)

\end{lstlisting}

Warstwa ta działa tylko podczas szkolenia i nie wpływa na obrazy przekazane w celu sprawdzenia skuteczności modelu, czy w celu klasyfikacji. Poniżej przedstawiono przykładowy wygląd losowego zdjęcia, po dokonaniu losowych przekształceń przez warstwe przedstawioną w kodzie \ref{lst:paugmentacja} .

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/wykres-augmentacji.png}
	\caption{	Przykładowe zdjęcie ze zbioru danych po poddaniu go augmentacji danych.}
	\label{fig:zdjecia_lisci}
\end{figure}
\begin{verbatim}

\end{verbatim}
\subsection{Metryki ewaluacji wyników}
Ocena skuteczności modeli klasyfikacji obrazów wymaga
zastosowania wielorakich metryk ewaluacyjnych w celu
uzyskania obiektywnego wyniku skuteczności i jakości
modelu przez nas badanego. Odpowiedni dobór metryk
pozwala na szeroką interpretacje otrzymanych wyników,
także przy obecności niezbalansowanych danych. Pozwalają
na analizę ogólną modelu, a także na identyfikację błędów,
takich jak nieodpowiedni dobór parametrów, czy zbioru
szkoleniowego. Poniżej przedstawiono szereg metryk
ewaluacyjnych wybranych z zamiarem użycia ich w badaniu,
wraz ze wzorami oraz krótkim opisem użycia . Są to metryki
szeroko stosowane w innych pracach z badanej tematyki, co
pozwoli dodatkowo porównać otrzymane wyniki do innych
prac w dziedzinie.
\subsubsection{Macierz pomyłek}
Macierz pomyłek (ang.confusin matrix)  stanowi podstawową metrykę ewaluacyjną modeli klasyfikacyjnych, której wyniki są używane następnie do wyliczenia bardziej złożonych metryk pozwalających na obiektywniejszą ocenę modelu. Jest to tabela krzyżowa pozwalająca na porównanie wyników predykcji do oryginalnego przypisania klas do danych. Dzieli ona wynik klasyfikacji na cztery kategorię: TP(true positivie), TN(true negative), FP( false positive) i  FN(false negative) \cite{grandini2020metrics}. Dla klasyfikacji wieloklasowej, macierz pomyłek, jest macierzą o wymiarach n x n, gdzie n to liczba klas, co przedstawiono w przykładzie dla klasy ‘B’ poniżej.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{img/macierz_pomylek.png}
	\caption{Przykładowa macierz pomyłek dla trzech klas, z wynikami otzymywanymi dla klasy ‘B’.}
	\label{fig:macierz_konfliktu}
\end{figure}

\subsubsection{Dokładność}
Wzór na Dokładność (ang.Accuracy) dzieli sume poprawnie przypisanych przez model klas(TP i TN) przez sumę wszystkich dokonanych klasyfikacji (TP +TN+ FN+ FP).\cite{grandini2020metrics} 

$$Accuracy = \frac{TP+TN}{TP+TN+FN+FP}$$

Jest więc to prawdopodobieństwo dokonania poprawnej predykcji przez model. Jest to podstawowa metryka, pozwalająca na szybką ocenę skuteczności modeli, zwracająca skuteczność predykcji modelu na całym zbiorze danych, nie bierze jednak pod uwagę niezbalansowania zbioru danych. Jest to jednak prosta w zrozumieniu metryka pozwalająca na szybka i wstępną ocenę modelu.

\subsubsection{Precyzja}
Precision to miara informująca nas, jaki procent przypisań danej klasy dokonanych przez model, zgadza się z oryginalnymi przypisaniami. Innymi słowy, z jaką precyzją model dokonuję predykcji danej klasy. \cite{grandini2020metrics}

$$Precision= \frac{TP}{TP+FN}$$

Miara ta jest używana dla konkretnych klas, jednak jej średnia, może zostać użyta do oceny całego modelu. Plusem jednak wyliczenia dla każdej klasy tej miary jest szersza ocena skuteczności modelu, co skutkować może wykryciem np. niezbalansowania klasy

\subsubsection{Czułość}
Czułość(ang. recall) to miara informująca nas, jaki procent danych, które oryginalnie mają daną klasę, zostało poprawnie sklasyfikowanych przed model. Jest więc to liczba poprawnych przypisań danej klasy(TP) podzielona przez liczbę danych mającej tą klasę w zbiorzę(TP+FN). \cite{grandini2020metrics}

$$Recall  = \frac{TP}{TP+TN}$$

Miara ta pozwala na stwierdzenie, czy model jest wstanie poprawnie przypisać klasę do danych. Możliwe jest też użycie średniej tej metryki, co pozwala na uzyskanie oceny całego modelu, jednak pozbawia nas możliwości zbadania klasyfikacji poszczególnych klas.

\subsubsection{Miara F1}
Miara F1(ang. F1-Score) jest próbą znalezienia pojedynczej metryki, pozwalającej na jak bardziej obiektywną ocenę modelu. Metryka agreguje metryki Precyzji i Czułości z pomocą miary harmonicznej. \cite{grandini2020metrics}

$$F1-Score= 2*(\frac{precision*recall}{precision+recall})$$

Przedstawiony wyżej wzór jest używany tylko w ramach klasyfikacji binarnej. Aby użyć tej miary w ramach klasyfikacji wieloklasowej należy, obliczyć odpowiednie średnie wcześniej wykorzystanych miar. W ramach tego otrzymujemy dwie nowe, ogólne miary: Macro F1-Score, które wykorzystuje arytmetyczną średnią Precyzji i Czułości, oraz Micro F1-Score – wykorzystująca średnie harmoniczną.\cite{grandini2020metrics}

$$Macro F1-Score= 2*(\frac{Arithmetic Precision* Arithmetic Recall}{Arithmetic Precision+Arithmetic Recall})$$
$$ Micro F1-Score= 2*(\frac{Harmonic Precision* Harmonic Recall}{Harmonic Precision+Harmonic Recall})$$
