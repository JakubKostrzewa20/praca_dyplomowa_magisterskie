\newpage
\section{Metodyka badań}
Celem poniższego rozdziału jest przedstawienie specyfikacji sprzetowej, metod, narzędzi oraz przeprowadzonych badań porównawczych modeli głębokiego nauczania.
\subsection{Specyfikacja sprzętowa i środowiska programistycznego}
Wszystkie szkolenia oraz testowanie modeli zostały wykonane na sprzęcie o tych samych specyfikacjach, w celu zachowania obiektywności ekseprymentu. W badaniach wykorzystano komputer osobisty z konkretnymi specyfikacjami:
\begin{itemize}
\item Karta graficzna
	\begin{itemize}
		\item Model:NVIDIA GeForce RTX 3050 Ti Laptop GPU
		\item Pamięć VRAM: 4 GB
		\item Obsługa CUDA: CUDA 13.0
	\end{itemize}
\item Procesor CPU: 
\begin{itemize}
	\item Model: AMD Ryzen 5 5600H with Radeon Graphics
	\item Liczba rdzeni fizycznych CPU: 6
	\item Wątki logiczne: 12
\end{itemize}
\item System operacyjny:
\begin{itemize}
	\item Nazwa systemu:Windows 11 Home
	\item Wersja systemu:21H2
	\item Typ systemu:64-bitowy system operacyjny
\end{itemize}
\item Pamięć RAM : 16,0 GB
\end{itemize}
Do treningu oraz ewaluacji modeli wykorzystano konkretne narzędzia oraz biblioteki języka Python:
\begin{itemize}
	\item Python
\end{itemize}
\subsection{Lista modeli wykorzystana w badaniu}
Wybrane rodziny oraz modele pozwalają na szeroki zakres badania różnych sposobów oraz cech modeli. Wybrane modele pochodzą z rodzin opisanych w roz. . 
Poniżej przedstawiono listę modeli wybranych do badania architektur:
\begin{table}[H]
	\begin{tabular}{|l|l|l|l|}
		\hline
		Rodzina modeli & Nazwa modelu     & Liczba parametrów & Typ architektury \\ \hline
		ResNet         & ResNet50         & 1                 & CNN              \\ \hline
		ResNet         & ResNet102        & 2                 & CNN              \\ \hline
		ViT          & ViT-S/16         & 21,677,214                 & VT               \\ \hline
		ViT          & ViT-B/16         & 4                 & VT               \\ \hline
		MobileNet      & MobileNetV2      & 2,929,246                 & CNN              \\ \hline
		MobileNet      & MobileNetV3Small & 6                 & CNN              \\ \hline
		EfficientNet   & EfficientNetB0   & 7                 & CNN              \\ \hline
		EfficientNet   & EfficientNetB3   & 8                 & CNN              \\ \hline
	\end{tabular}
\end{table}
\subsection{Strategia treningu}
W ramach treningu dokonano fine-tunningu osiemu wcześniej przedstawionych modeli.
Aby otrzymać obiektywne między modelami wyniki testów, modele zostały wyszkolone w ten sam sposób.
Każdy z modeli działał na tym samym zbiorze treningowym, testowym oraz walidacyjnym.
Dla każdego szkolenia zastosowano tą samą konfiguracje, a długość szkolenia, choć z góry ustalona, była też kontrolowana techniką EarlyStopping. Podejście to pozwoliło na obiektywną ocene skuteczności poszczególnych modeli.
\subsection{Konfiguracja modeli}
Niniejszy podrozdział przedstawia ogólną konfigurację modeli wykorzystaną w przeprowadzonym eksperymencie. Opisano w nim przeprowadzoną augmentacje danych, wybrane parametry oraz zastosowane techniki w ramach przeprowadzonego eksperymentu. Zastosowanie jednolitej konfiguracji pozwala na bardziej rzetelne i obiektywniejsze porównanie badanych modeli. 
\subsubsection{Augmentacja danych}
Z powodu niezbalansowanego zbioru danych zastosowano augmentacje danych, mającą na celu zapobieganie przeuczeniu. Zdjęcia dostarczane do modeli w ramach zbioru treningowego są poddawane przekształceniom, takim jak:
\begin{itemize}
	\item RandomFlip - warstwa odpowiadająca za losową rotację obrazów;
	\item RandomFlip - warstwa odpowiadająca za odwracanie obrazów;
	\item RandomConstrast - warstwa losowo zmieniająca kontrast obrazów;
	\item RandomZoom - warstwa odpowiadająca za losowe przybliżanie obrazów.
\end{itemize}
Poniżej przedstawiono kod warstwy odpowiadającej za augmentacje: 
\begin{lstlisting}[language=Python, caption={Kod warstwy odpowiadającej za augmentacje danych}, label={lst:paugmentacja}]
data_augmentation = tf.keras.Sequential(
	[
	layers.RandomFlip("horizontal"),
	layers.RandomRotation(0.2),
	layers.RandomContrast(0.2),
	layers.RandomZoom(0.2),
	]
)
\end{lstlisting}
Warstwa ta działa tylko podczas szkolenia i nie wpływa na obrazy przekazane w celu sprawdzenia skuteczności modelu, czy w celu klasyfikacji. Poniżej przedstawiono przykładowy wygląd losowego zdjęcia, po dokonaniu losowych przekształceń przez warstwe przedstawioną w kodzie \ref{lst:paugmentacja}.
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/wykres-augmentacji.png}
	\caption{Przykładowe zdjęcie ze zbioru danych po poddaniu go augmentacji danych.}
	\label{fig:zdjecia_lisci}
\end{figure}
\begin{verbatim}

\end{verbatim}
\subsubsection{Parametry}
Poniższe parametry zostały zastosowane w ramach szkolenia dla wszystkich badanych modeli:
\begin{itemize}
	\item Optimizer: Adam
	\item Epochs: 50
	\item Batch\_size: 16
	\item Rozmiar zdjęć: 224x224
	\item Learning rate: 0.001
\end{itemize}
\subsubsection{Early Stopping}
EarlyStopping to funkcja wbudowana w TensorFlow pozwalająca na zatrzymanie szkolenia w momencie zaobserwowania braku poprawy metryki loss przez pięć następujących po sobie epok. Pozwala to na zapobiegnięcie overfittingowi oraz oszczędzanie zasobów obliczeniowych. 
\begin{lstlisting}[language=Python, caption={Kod implementujący EarlyStopping w ramach szkolenia}, label={lst:earlystopping}]
callback = tf.keras.callbacks.EarlyStopping(monitor="loss", patience=5)
\end{lstlisting}
\subsection{Metryki ewaluacji wyników}
Ocena skuteczności modeli klasyfikacji obrazów wymaga
zastosowania wielorakich metryk ewaluacyjnych w celu
uzyskania obiektywnego wyniku skuteczności i jakości
modelu przez nas badanego. Odpowiedni dobór metryk
pozwala na szeroką interpretacje otrzymanych wyników,
także przy obecności niezbalansowanych danych. Pozwalają
na analizę ogólną modelu, a także na identyfikację błędów,
takich jak nieodpowiedni dobór parametrów, czy zbioru
szkoleniowego. Poniżej przedstawiono szereg metryk
ewaluacyjnych wybranych z zamiarem użycia ich w badaniu,
wraz ze wzorami oraz krótkim opisem użycia . Są to metryki
szeroko stosowane w innych pracach z badanej tematyki, co
pozwoli dodatkowo porównać otrzymane wyniki do innych
prac w dziedzinie.
\subsubsection{Macierz pomyłek}
Macierz pomyłek (ang.confusin matrix)  stanowi podstawową metrykę ewaluacyjną modeli klasyfikacyjnych, której wyniki są używane następnie do wyliczenia bardziej złożonych metryk pozwalających na obiektywniejszą ocenę modelu. Jest to tabela krzyżowa pozwalająca na porównanie wyników predykcji do oryginalnego przypisania klas do danych. Dzieli ona wynik klasyfikacji na cztery kategorię: TP(true positivie), TN(true negative), FP( false positive) i  FN(false negative) \cite{grandini2020metrics}. Dla klasyfikacji wieloklasowej, macierz pomyłek, jest macierzą o wymiarach n x n, gdzie n to liczba klas, co przedstawiono w przykładzie dla klasy ‘B’ poniżej.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{img/macierz_pomylek.png}
	\caption{Przykładowa macierz pomyłek dla trzech klas, z wynikami otzymywanymi dla klasy ‘B’.}
	\label{fig:macierz_konfliktu}
\end{figure}

\subsubsection{Dokładność}
Wzór na Dokładność (ang.Accuracy) dzieli sume poprawnie przypisanych przez model klas(TP i TN) przez sumę wszystkich dokonanych klasyfikacji (TP +TN+ FN+ FP).\cite{grandini2020metrics} 

$$Accuracy = \frac{TP+TN}{TP+TN+FN+FP}$$

Jest więc to prawdopodobieństwo dokonania poprawnej predykcji przez model. Jest to podstawowa metryka, pozwalająca na szybką ocenę skuteczności modeli, zwracająca skuteczność predykcji modelu na całym zbiorze danych, nie bierze jednak pod uwagę niezbalansowania zbioru danych. Jest to jednak prosta w zrozumieniu metryka pozwalająca na szybka i wstępną ocenę modelu.

\subsubsection{Precyzja}
Precision to miara informująca nas, jaki procent przypisań danej klasy dokonanych przez model, zgadza się z oryginalnymi przypisaniami. Innymi słowy, z jaką precyzją model dokonuję predykcji danej klasy. \cite{grandini2020metrics}

$$Precision= \frac{TP}{TP+FN}$$

Miara ta jest używana dla konkretnych klas, jednak jej średnia, może zostać użyta do oceny całego modelu. Plusem jednak wyliczenia dla każdej klasy tej miary jest szersza ocena skuteczności modelu, co skutkować może wykryciem np. niezbalansowania klasy

\subsubsection{Czułość}
Czułość(ang. recall) to miara informująca nas, jaki procent danych, które oryginalnie mają daną klasę, zostało poprawnie sklasyfikowanych przed model. Jest więc to liczba poprawnych przypisań danej klasy(TP) podzielona przez liczbę danych mającej tą klasę w zbiorzę(TP+FN). \cite{grandini2020metrics}

$$Recall  = \frac{TP}{TP+TN}$$

Miara ta pozwala na stwierdzenie, czy model jest wstanie poprawnie przypisać klasę do danych. Możliwe jest też użycie średniej tej metryki, co pozwala na uzyskanie oceny całego modelu, jednak pozbawia nas możliwości zbadania klasyfikacji poszczególnych klas.

\subsubsection{Miara F1}
Miara F1(ang. F1-Score) jest próbą znalezienia pojedynczej metryki, pozwalającej na jak bardziej obiektywną ocenę modelu. Metryka agreguje metryki Precyzji i Czułości z pomocą miary harmonicznej. \cite{grandini2020metrics}

$$F1-Score= 2*(\frac{precision*recall}{precision+recall})$$

Przedstawiony wyżej wzór jest używany tylko w ramach klasyfikacji binarnej. Aby użyć tej miary w ramach klasyfikacji wieloklasowej należy, obliczyć odpowiednie średnie wcześniej wykorzystanych miar. W ramach tego otrzymujemy dwie nowe, ogólne miary: Macro F1-Score, które wykorzystuje arytmetyczną średnią Precyzji i Czułości, oraz Micro F1-Score – wykorzystująca średnie harmoniczną.\cite{grandini2020metrics}

$$Macro F1-Score= 2*(\frac{Arithmetic Precision* Arithmetic Recall}{Arithmetic Precision+Arithmetic Recall})$$
$$ Micro F1-Score= 2*(\frac{Harmonic Precision* Harmonic Recall}{Harmonic Precision+Harmonic Recall})$$